{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## PDF Summarization"
      ],
      "metadata": {
        "id": "awvwUb3Qw2zq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "06wNVgDyp_RT"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets accelerate -U\n",
        "!pip install evaluate rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "raw_datasets = load_dataset(\"knkarthick/samsum\")\n",
        "print(\"Dataset successfully loaded!\")\n",
        "\n",
        "MAX_INPUT_LENGTH = 512\n",
        "MAX_TARGET_LENGTH = 128\n",
        "PREFIX = \"summarize: \""
      ],
      "metadata": {
        "collapsed": true,
        "id": "SR2T5t_jqjqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = [PREFIX + doc for doc in examples[\"dialogue\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"summary\"], max_length=MAX_TARGET_LENGTH, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(500))\n",
        "small_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(50))\n",
        "print(f\"Using {len(small_train_dataset)} samples for training.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3zaq8RAcrime"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
        "\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    return {k: round(v, 4) for k, v in result.items()}\n",
        "\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./t5_summarization_results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=True,\n",
        "    report_to=\"none\",\n",
        "    predict_with_generate=True,\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ogZ46uNarzAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"  Starting T5 Summarization Fine-Tuning...\")\n",
        "print(\"  Watch the Loss and ROUGE metrics update below.\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Fine-Tuning Complete! Model is now saved.\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "trainer.save_model(\"./final_t5_summarizer\")\n",
        "tokenizer.save_pretrained(\"./final_t5_summarizer\")"
      ],
      "metadata": {
        "id": "fZqqbIMnsMyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_eQ1aH4hvpnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import fitz\n",
        "\n",
        "print(\"Please upload your PDF file now:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    PDF_FILE_NAME = list(uploaded.keys())[0]\n",
        "    print(f\"File '{PDF_FILE_NAME}' detected and uploaded.\")\n",
        "else:\n",
        "    print(\"No file uploaded. Please re-run the cell and upload a PDF.\")\n",
        "    PDF_FILE_NAME = None\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    if not pdf_path:\n",
        "        return None\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        for page in doc:\n",
        "            text += page.get_text() + \"\\n\"\n",
        "        doc.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading PDF: {e}\")\n",
        "        return None\n",
        "    return text.strip()\n",
        "\n",
        "if PDF_FILE_NAME:\n",
        "    long_text = extract_text_from_pdf(PDF_FILE_NAME)\n",
        "\n",
        "    if long_text:\n",
        "        print(f\"Successfully extracted {len(long_text)} characters.\")\n",
        "    else:\n",
        "        print(\"Could not extract text. Check file name and format.\")\n",
        "else:\n",
        "    long_text = None"
      ],
      "metadata": {
        "id": "yWGoEAAfv254"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "MODEL_PATH = \"./final_t5_summarizer\"\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH).to(\"cuda\")\n",
        "    PREFIX = \"summarize: \"\n",
        "    MAX_INPUT_LENGTH = 512\n",
        "    print(f\"Model '{MODEL_PATH}' loaded successfully.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    print(\"Please ensure you ran the fine-tuning cell and it completed successfully.\")\n",
        "    model = None"
      ],
      "metadata": {
        "id": "z4pa3ghVv-Eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(text, model, tokenizer, max_input=MAX_INPUT_LENGTH):\n",
        "    if not model or not text:\n",
        "        return \"Model not loaded or no text extracted.\"\n",
        "    input_text = PREFIX + text\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        input_text,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=max_input,\n",
        "        truncation=True\n",
        "    ).to(model.device)\n",
        "\n",
        "    summary_ids = model.generate(\n",
        "        inputs.input_ids,\n",
        "        num_beams=4,\n",
        "        max_length=150,\n",
        "        min_length=30,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    summary = tokenizer.decode(\n",
        "        summary_ids.squeeze(),\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=True\n",
        "    )\n",
        "\n",
        "    return summary\n",
        "\n",
        "if long_text and model:\n",
        "    final_summary = generate_summary(long_text, model, tokenizer)\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"       SUMMARY FOR THE DOCUMENT: {PDF_FILE_NAME}\")\n",
        "    print(\"=\"*70)\n",
        "    print(final_summary)\n",
        "    print(\"=\"*70)\n",
        "elif not model:\n",
        "    print(\"\\nCannot run summarization: Model failed to load.\")\n",
        "elif not long_text:\n",
        "    print(\"\\nCannot run summarization: No text was successfully extracted from the PDF.\")"
      ],
      "metadata": {
        "id": "PIMxF-T7wApD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}