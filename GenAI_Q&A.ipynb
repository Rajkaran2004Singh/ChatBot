{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vuZ92NTJETJ9"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets accelerate -U\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "raw_datasets = load_dataset(\"squad\", split=\"train[:5000]\")\n",
        "raw_datasets = raw_datasets.train_test_split(test_size=0.1)\n",
        "\n",
        "print(f\"Loaded {len(raw_datasets['train'])} training examples.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kRQPbncVE_SF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 384\n",
        "STRIDE = 128\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=MAX_LENGTH,\n",
        "        truncation=\"only_second\",\n",
        "        stride=STRIDE,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    inputs[\"start_positions\"] = []\n",
        "    inputs[\"end_positions\"] = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        sample_index = sample_map[i]\n",
        "        answer = examples[\"answers\"][sample_index]\n",
        "        context = examples[\"context\"][sample_index]\n",
        "\n",
        "        if len(answer['text']) == 0:\n",
        "            inputs[\"start_positions\"].append(0)\n",
        "            inputs[\"end_positions\"].append(0)\n",
        "            continue\n",
        "\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = start_char + len(answer[\"text\"][0])\n",
        "\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        context_start_token = sequence_ids.index(1)\n",
        "        context_end_token = len(sequence_ids) - 1\n",
        "\n",
        "        start_token = context_start_token\n",
        "        while start_token < context_end_token and offsets[start_token][0] <= start_char:\n",
        "            start_token += 1\n",
        "        inputs[\"start_positions\"].append(start_token - 1)\n",
        "\n",
        "        end_token = context_end_token\n",
        "        while end_token >= context_start_token and offsets[end_token][1] >= end_char:\n",
        "            end_token -= 1\n",
        "        inputs[\"end_positions\"].append(end_token + 1)\n",
        "\n",
        "    return inputs\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"train\"].column_names\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9oKCDN7_Fn0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, default_data_collator\n",
        "import torch\n",
        "\n",
        "use_fp16 = False\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qa_bert_results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\",\n",
        "    fp16=use_fp16,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"].select(range(500)),\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=default_data_collator,\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"  Starting BERT Q&A Fine-Tuning\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\nFine-Tuning Complete! Model is now saved.\")\n",
        "trainer.save_model(\"./final_bert_qa_model\")\n",
        "tokenizer.save_pretrained(\"./final_bert_qa_model\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1rbPMl_vGD0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoModelForQuestionAnswering, AutoTokenizer\n",
        "\n",
        "MODEL_PATH = \"./final_bert_qa_model\"\n",
        "qa_pipeline = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=AutoModelForQuestionAnswering.from_pretrained(MODEL_PATH),\n",
        "    tokenizer=AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        ")\n",
        "\n",
        "def answer_question_from_pdf(question, context):\n",
        "    if not context:\n",
        "        return \"Error: Context (PDF text) is empty.\"\n",
        "\n",
        "    result = qa_pipeline({\n",
        "        'question': question,\n",
        "        'context': context\n",
        "    })\n",
        "\n",
        "    print(\"\\n--- Q&A Result ---\")\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Predicted Answer: {result['answer']}\")\n",
        "    print(f\"Confidence Score: {result['score']:.4f}\")\n",
        "    print(\"------------------\")\n",
        "\n",
        "    return result['answer']\n",
        "\n",
        "sample_context = \"\"\"\n",
        "BERT was invented by Google and uses a Transformer architecture. The model\n",
        "fine-tuned here was only trained for a short period of 3 epochs to conserve\n",
        "time, but is now ready for demonstration.\n",
        "\"\"\"\n",
        "q_test = \"How long was the model trained for?\"\n",
        "answer_question_from_pdf(q_test, sample_context)"
      ],
      "metadata": {
        "id": "dfBan2r9IV0i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}