{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MgIsxkKcJvHt"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets accelerate peft bitsandbytes\n",
        "!pip install evaluate rouge-score\n",
        "!pip install Pillow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "MODEL_NAME = \"Salesforce/blip-image-captioning-base\"\n",
        "processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
        "model = BlipForConditionalGeneration.from_pretrained(MODEL_NAME).to(\"cuda\")\n",
        "\n",
        "print(\"Loading 'pokemon-blip-captions' dataset subset...\")\n",
        "try:\n",
        "    raw_dataset = load_dataset(\"lambdalabs/pokemon-blip-captions\", split=\"train[:100]\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading main dataset: {e}. Falling back to 'conceptual_captions' subset.\")\n",
        "    raw_dataset = load_dataset(\"conceptual_captions\", split=\"train[:100]\")\n",
        "\n",
        "raw_dataset = raw_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "print(f\"Loaded {len(raw_dataset['train'])} training examples.\")\n",
        "print(f\"Loaded {len(raw_dataset['test'])} evaluation examples.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8EB6r-KvKLDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "class ImageCaptioningDataset(Dataset):\n",
        "    def __init__(self, dataset, processor):\n",
        "        self.dataset = dataset\n",
        "        self.processor = processor\n",
        "        self.image_url_key = 'image_url'\n",
        "        self.caption_key = 'caption'\n",
        "\n",
        "        if self.image_url_key not in dataset.column_names or self.caption_key not in dataset.column_names:\n",
        "             raise ValueError(\"Dataset columns missing confirmed keys ('image_url' and 'caption').\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        image_url = item[self.image_url_key]\n",
        "        caption = item[self.caption_key]\n",
        "\n",
        "        try:\n",
        "            response = requests.get(image_url, timeout=10)\n",
        "            if response.status_code != 200:\n",
        "                raise ConnectionError(f\"Failed to download image from {image_url}\")\n",
        "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "        except Exception:\n",
        "            return self.__getitem__((idx + 1) % len(self.dataset))\n",
        "\n",
        "        encoding = self.processor(\n",
        "            images=image,\n",
        "            text=caption,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        encoding = {k: v.squeeze() for k, v in encoding.items()}\n",
        "        encoding[\"labels\"] = encoding.pop(\"input_ids\")\n",
        "\n",
        "        return encoding\n",
        "\n",
        "try:\n",
        "    train_dataset = ImageCaptioningDataset(raw_dataset['train'], processor)\n",
        "    eval_dataset = ImageCaptioningDataset(raw_dataset['test'], processor)\n",
        "except ValueError as e:\n",
        "    print(f\"Error creating dataset: {e}\")\n",
        "    raise\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=4)\n",
        "eval_dataloader = DataLoader(eval_dataset, shuffle=False, batch_size=4)\n",
        "\n",
        "print(\"Custom dataset and data loaders created.\")"
      ],
      "metadata": {
        "id": "fdjAT6B8K9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shift_tokens_right(input_ids, pad_token_id, decoder_start_token_id):\n",
        "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
        "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
        "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
        "\n",
        "    if pad_token_id is None:\n",
        "        raise ValueError(\"`pad_token_id` has to be defined.\")\n",
        "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
        "    return shifted_input_ids\n",
        "\n",
        "NUM_EPOCHS = 5\n",
        "LEARNING_RATE = 5e-5\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "for param in model.vision_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"  Starting BLIP Fine-Tuning for {NUM_EPOCHS} Epochs\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    total_loss = 0\n",
        "\n",
        "    for idx, batch in enumerate(train_dataloader):\n",
        "\n",
        "        pixel_values = batch.pop(\"pixel_values\").to(device)\n",
        "        labels = batch.pop(\"labels\").to(device)\n",
        "\n",
        "        decoder_input_ids = shift_tokens_right(\n",
        "            labels,\n",
        "            model.config.text_config.pad_token_id,\n",
        "            model.config.text_config.bos_token_id\n",
        "        )\n",
        "\n",
        "        outputs = model(\n",
        "            pixel_values=pixel_values,\n",
        "            input_ids=decoder_input_ids,\n",
        "            labels=labels,\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if idx % 10 == 0:\n",
        "            print(f\"Epoch {epoch} Batch {idx}/{len(train_dataloader)} Loss: {loss.item():.4f}\")\n",
        "\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} Complete --- Average Loss: {total_loss / len(train_dataloader):.4f}\")\n",
        "\n",
        "MODEL_PATH = \"./final_blip_captioner\"\n",
        "model.save_pretrained(MODEL_PATH)\n",
        "processor.save_pretrained(MODEL_PATH)\n",
        "print(f\"Fine-Tuning Complete! Model saved to {MODEL_PATH}.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "e516CRTfLFUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "from google.colab import files\n",
        "import torch\n",
        "import io\n",
        "\n",
        "MODEL_PATH = \"./final_blip_captioner\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Loading fine-tuned BLIP model from {MODEL_PATH}...\")\n",
        "try:\n",
        "    model = BlipForConditionalGeneration.from_pretrained(MODEL_PATH).to(device)\n",
        "    processor = AutoProcessor.from_pretrained(MODEL_PATH)\n",
        "    model.eval()\n",
        "    print(\"Model loaded successfully.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Failed to load model. Ensure Step 4 completed successfully and the directory exists. Error: {e}\")\n",
        "    raise\n",
        "\n",
        "print(\"Image Upload\")\n",
        "print(\"Please upload the image you want the model to caption:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if not uploaded:\n",
        "    print(\"No image uploaded. Skipping test.\")\n",
        "else:\n",
        "    image_filename = next(iter(uploaded))\n",
        "    image_data = uploaded[image_filename]\n",
        "\n",
        "    try:\n",
        "        sample_image = Image.open(io.BytesIO(image_data)).convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Could not open uploaded file as an image. Error: {e}\")\n",
        "        sample_image = None\n",
        "\n",
        "    if sample_image:\n",
        "        print(f\"Image '{image_filename}' uploaded successfully.\")\n",
        "        print(\"Model Test\")\n",
        "        display(sample_image)\n",
        "\n",
        "        inputs = processor(images=sample_image, return_tensors=\"pt\").to(device)\n",
        "        pixel_values = inputs.pixel_values\n",
        "\n",
        "        print(\"Generating caption...\")\n",
        "        generated_ids = model.generate(\n",
        "            pixel_values=pixel_values,\n",
        "            max_length=50,\n",
        "            num_beams=4\n",
        "        )\n",
        "\n",
        "        generated_caption = processor.decode(generated_ids.squeeze(), skip_special_tokens=True)\n",
        "\n",
        "        print(\"Generated Caption\")\n",
        "        print(f\"Fine-tuned Model Output: {generated_caption}\")"
      ],
      "metadata": {
        "id": "Xgr3uG3xObLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import AutoPipelineForText2Image, EulerDiscreteScheduler\n",
        "import torch\n",
        "from IPython.display import display\n",
        "\n",
        "MODEL_NAME = \"runwayml/stable-diffusion-v1-5\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Loading Text-to-Image pipeline with model: {MODEL_NAME} on device: {device}...\")\n",
        "\n",
        "try:\n",
        "    pipe = AutoPipelineForText2Image.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float16\n",
        "    ).to(device)\n",
        "    pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "    print(\"Pipeline loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Failed to load the model. Ensure you have a GPU runtime enabled. Error: {e}\")\n",
        "    if device == \"cuda\":\n",
        "        print(\"Attempting to load on CPU with float32 as a fallback...\")\n",
        "        pipe = AutoPipelineForText2Image.from_pretrained(MODEL_NAME).to(\"cpu\")\n",
        "        pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "        device = \"cpu\"\n",
        "        print(\"Pipeline loaded on CPU (performance will be slow).\")\n",
        "    else:\n",
        "        raise\n",
        "\n",
        "print(\"\\nGenerate Your Image\")\n",
        "\n",
        "user_prompt = input(\"Enter your image description : \")\n",
        "\n",
        "if not user_prompt.strip():\n",
        "    user_prompt = \"A simple abstract painting\"\n",
        "    print(f\"No input detected. Using default prompt: '{user_prompt}'\")\n",
        "\n",
        "print(f\"Generating image for prompt: '{user_prompt}'\")\n",
        "\n",
        "image = pipe(\n",
        "    prompt=user_prompt,\n",
        "    num_inference_steps=25,\n",
        "    guidance_scale=7.5\n",
        ").images[0]\n",
        "\n",
        "print(\"\\nImage Generated!\")\n",
        "display(image)"
      ],
      "metadata": {
        "id": "O0dSQUI1myx-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}